# VideoScriptsNotepad
A place to keep ideas that may or may not be ready for future videos.


There are no gaps in a text file. Every character is right next to another. There is no way to put a gap between characters in a text file. Even null is a character, as is space, and new-line. You can’t have some text, a gap, and some more text. This is an inherent limitation of text files. However, CSV is a way of treating a text file like a logical grid of cells. It stands for “Comma Separated Values” and each cell is represented by the characters between the commas. If there is no text between commas, that represents an empty spot in the grid. CSV files don’t have any gaps in them, but they represent a grid of cells that can. 

Anywhere a wire goes, the electromagnetic wave traveling along it goes, along with the bit it represents. This is true even when the wire splits and the wave goes down multiple paths. This allows multiple components to all read the same value from the wire. Other components, though, have no access to that value since they are not connected to the wire. This forces hardware designers to treat data as something that needs to be transferred from component to component via set pathways. The electromagnetic wave is guided down the wire but is not radiated out into the atmosphere like light from a lightbulb. If it were, hardware designers would have access to that bit from any other component nearby, not just those connected to the wire. 

Your cell phone receives the signals of all the other cell phones in the area but does nothing with them. The signal from your cell phone is only acted upon by the nearby cell tower. It then responds to your cell phone but everyone else nearby receives the signal as well but ignores it. Cell networks mimic point to point phone lines using broadcast devices. Therefore, the compute to signal ratio is extremely low. If there were a network of broadcast devices that could hear and respond to all their neighbors in a way that made computational sense, there would be much more computational power that could be expressed in this network compared to current cell networks. 

We used to think hardware is concrete but software is purely abstract. The wires are things but the bits they represent are just ideas. However, now we say things like “program to an abstraction, not a concrete implementation”. The concrete implementation is just something purely in the software though, so how can it be concrete? Our realization of how far down we have to dig to truly find something abstract has been continually expanding. Much like command query separation (CQS) has helped developers better reason about their code, concrete abstract separation (CAS) can do the same. 

Encoding the same programming concept in different programming languages prevents reusability and is the biggest violator of the famous Don't Repeat Yourself principle. It’s akin to the use of different text encoding schemes prior to the adoption of ASCII, and eventually, Unicode. They all repeatedly referenced the same underlying concepts, just in different ways. By making those underlying concepts fit a uniform reference system, though, repetitive programming tasks related to text decoding were virtually eliminated. The same thing could happen for programming concepts themselves, not just the text encoding scheme used to write the programs. Abstracting out the concepts from the program and encoding them separately might allow a higher level treatment of programming above and beyond the language-specific methods we currently use.

Names and locations are the same thing, at least to a computer. If you change the name of a file, the location of that file changes. Any program trying to find that file using the old name will not find it, its path has changed. 

The Key Concept In Programming 
—-
Programming at every level uses comparison to determine which path to take through the code. The CPU can process instructions in any order from RAM but, by default, it processes them in order. The only time it doesn’t is when an instruction modifies the next instruction address. This next instruction address may be represented by a name in a higher level programming language, like a function name. This function may not need to be called every time the program is run, though. Maybe only if the user types a particular letter should it run, such as the letter “y.” How do you know if they’ll type that letter, though, when you’re writing the program? You don’t. You have to write instructions that can handle either case. To know which path to take you tell the CPU to compare the user's input with what you expect. The CPU compares them using subtraction. Every byte in RAM can be treated like a number so any two bytes in RAM can be compared using subtraction. If the answer is zero then the two numbers are the same. Otherwise if the answer is positive or negative you know which input is larger. Depending upon the answer, your program tells the CPU to jump to the appropriate function. The subtraction machine in the CPU sets flags when the compare instruction is processed and then the control unit uses those flags when the jump-if instruction is processed afterwards. This combination of a compare instruction with a jump-if instruction immediately afterwards is so common in programming that you couldn’t do it without them.



Atoms have protons and neutrons in their nucleus along with electrons around it. The number of protons in the atom determines what type of element it is. It also determines how many electrons the atom attracts. Each proton attracts 1 electron. Any atom that has less electrons than protons creates a positive electric field around itself which attracts electrons. An electron has a negative electric field around it which is attracted to the positive field of the unbalanced atom. This is much like what happens with magnets. However you can’t separate the positive and negative parts of a magnet like you can with positive and negative electric charges. Magnets also continue to attract other magnets even after you group them together. Positive and negative electric charges neutralize each other. When the number of protons and electrons in the atom balances out then the electric field surrounding the atom effectively disappears so no more electrons are attracted to it.  

Electrons stack up around an atom in the same positions no matter what type of element it is. For instance, an atom of titanium has 23 electrons but the first 13 of those electrons are in the same positions around the atom as an element of aluminum which only has 13 electrons. The potential positions these electrons can take are called orbitals. These orbitals are grouped together into shells which exist at different levels around the atom. Each shell can hold a certain number of electron orbitals. Once a shell is full, electrons begin to fill up orbitals in the next outer shell around the atom. However many shells an atom has, the outermost shell is the most important. If all the spots in it are filled, the atom is stable. If the outermost shell is not full, then the electrons in that outermost shell are unstable and may occasionally move to another atom. This causes a gap in the outer shell of the original atom which creates a positive electric field around that atom and attracts a negative electron from somewhere else. If you can get electrons to move from atom to atom in a steady pattern, you can create an electric current. Atoms that have a full outermost shell don’t lose electrons to other atoms so an electric current cannot be created. These elements are called insulators. 

An electron has a negative electric field around it, but if it’s moving, it also has a magnetic field around it. This is why a wire with current flowing through it can be wrapped around a magnet to increase the magnetic field and produce an electromagnet. If the current stops, the electromagnetic effect goes away. The electrons are no longer moving so their magnetic field has disappeared. If an electron is not only moving, but accelerating, then the electric field and magnetic field around the electron interact and create an electromagnetic wave. The wave ends as soon as the electron stops accelerating. It’s much like the wave produced by your hand when you crack a whip. Your hand has to accelerate up to produce an upward wave in the whip and then accelerate back down to produce a downward wave which interact perfectly to produce the whipping sound. Acceleration produces waves, not just movement. 

Electromagnetic waves produced by a power source like a battery can continuously flow through a wire. Depending upon the power source, batteries or wall outlets, the waves can flow in one direction or alternate frequently between both directions. Since electrons move in the presence of an electric field, the electromagnetic wave causes a current of electrons to slowly drift in one or both directions. Removing or adding electrons at one end of a wire, by hooking it up to one terminal of a battery for instance, can also cause a single electromagnetic wave to pass through the wire and change the average ratio of electrons and protons to something other than 1 to 1. When that happens, the whole wire has an electric field and can attract or repel electrons nearby such as those in a semiconductor. That semiconductor material may be in a transistor between its source and its drain. The wire, called the gate here, can have a low enough ratio of electrons to protons that it creates an electric field big enough and strong enough to pull electrons from deep in the semiconductor up towards it. These electrons then collect at the top and get repelled by electrons in the source towards the drain. When there are enough electrons at the top, they allow a flow of electric current to start. The current keeps flowing as long as the gate maintains a strong enough electric field. 



Let’s say you have a circle with an arrow pinned to the middle of it. How many different directions can the arrow be pointing? Let’s say someone else has the same setup and you want to tell them which direction your arrow is pointing. That’s pretty hard to do unless you have a common reference point. To do that, you could break up the circle into an even number of sections like degrees on a compass or hours on a clock. By splitting a continuous thing like the circle into discrete sections, communication becomes easier and more reliable. Let’s say each section represents a word or a number. If it were a binary number you would only need two sections on your circle, 0 and 1. You could then encode data using a stream of these binary numbers and send them to the other person by controlling the direction of their arrow while they watch it. It would take you longer to send information this way since you are only able to deal with two possible values at a time, but as long as you could figure out how to encode everything you needed to send using binary then you could convey anything you wanted to say. You’d also have a lot of room for error since all you need to do is clearly communicate one value or the other. This is how morse code originally worked, except with a switch controlling another switch using electric current running through a wire. Computers do that same thing except with voltage levels on a wire. It doesn’t matter what the specific voltage level is on the wire, only whether it is above or below a certain threshold. Usually the voltage is way above or way below that threshold so there isn’t any confusion as to what the value is. This concept is called digital communication because it breaks up a continuous range of possible values into discrete values, and it’s also binary communication because it only uses 2 discrete values, 0 and 1. These binary digits are called bits for short.

Bits can be copied not only from component to component, but from one form to another. For instance, switches can be flipped one way or the other. A voltage can be present on a wire or not. Capacitors inside RAM can be charged or not. Each of these forms stores bits in different ways. A bit just means one of two possibilities. These possibilities are usually named 0 and 1 but they can be used to represent other things besides numbers. Anything that can be in two distinct states can represent a bit. The problem is when a bit is copied from one form into another, such as from a flipped switch to voltage on a wire to a charged capacitor in RAM, which state represents the 0 and which represents the 1? The key is making sure everyone knows which state of each device is mapped to which bit possibility. That’s where specifications from vendors are critical. Keeping the various forms of the bit consistent as it is copied from component to component maintains the illusion that there is an abstract thing called a bit moving around in the computer.

Configuration files are text files, just like programming files. There’s not really a configuration language like there is for programming though. Configuration files are more about creating a list of settings where each setting has a name and a value. How does the computer know where each setting is in the text file and how to tell what it’s name and value is, though?

Programming languages use names to refer to RAM addresses. At that RAM address could be an instruction or just some data. When you call a function, or save data to a variable, you’re simply using a synonym for a particular RAM address. 

Ultimately the data in RAM has no structure and is processed one instruction at a time. When you create classes in higher level programming languages, though, you are pretending that the data in RAM can have structures in it. The compiler perfectly coordinates the instructions it creates with the RAM addresses where it stores data giving you the illusion that RAM really can have structures inside. When you look at the CPU instructions produced by a compiler, you notice that they all just happen to access the starting address of the appropriate field or function from the right block of RAM at the right time as if there really were a structure there. That’s the trick of compilers. They allow you to think about RAM, and computers in general, from a higher level of abstraction by taking care of all the complex mapping between instructions and data for you. The trade off is that you have more strict rules about how you write programs than the freedom you have when writing all the instructions for the CPU yourself. In the end, they make programmers a lot more productive and the trade off is usually worth it. 


When you are constructing a new object somewhere in RAM and creating a pointer to the first address of that object, the compiler knows what it can do with that object based on the data type of the pointer.  You cannot interact with that object directly, only through a pointer. The data type of the pointer tells you what the object is capable of. If the data type of the pointer includes a more limited set of behavior than what the object is truly capable of, then you can’t use that pointer to trigger that other behavior. You can create a new pointer with a new data type that offers access to that additional behavior but you can’t copy the value of the first pointer into it without an error. The compiler needs you to confirm that you know what you’re doing by telling it that you really do want to cast this Employee in a Manager role, for instance. You know that employee started out as a manager before being implicitly cast as a regular employee. So you know you can cast that employee object as a manager again and it will be perfectly capable of doing the job. Remember, you can treat an object multiple ways, as an Employee or Manager in this case, but each pointer can only treat it one way. New pointers can be created that treat it differently but each pointer is static and cannot change.

Some compilers let you ignore data types when storing data or pointers. They let you think you can store whatever you want and change it whenever you want. This kind of dynamic typing means you don’t get the same compiler error messages as you do with a programming language that uses static typing. That can be nice when you really do know the program will run fine even though you get a compiler error. Static typing, though, can be nicer when you’re working on larger teams where no one can really anticipate how their code might be used. Static types act as guard rails that prevent certain errors from happening before the program ever runs. JavaScript, used heavily when building websites offers dynamic typing where the same label can refer to one data type and then another. Typescript was created to add static typing to JavaScript and is used heavily by large web development teams. 


to eventually produce an output. That output is initially reads through the bytes code point by code point, parses them into tokens which are just special objects in RAM with extra fields for each token, arranges those tokens conceptually into a root-like structure using pointers from one to another based upon their relationship - functions at the top with their inputs below, converts those hierarchies of tokens into CPU instructions while replacing function calls with symbolic names that can be linked up to a specific RAM address later, 


The concept of lazy loading where the initial call is costly but subsequent calls are just as efficient as non-lazy loading.

Copy on write, which is similar

It was possible in C to create structures in memory, called structs for short, that contained data fields and pointers just like objects. The difference was that these structures were data-oriented so the compiler wasn’t designed to treat the struct like a self-contained entity with its own behavior built in. This meant if you programmed in C with an object-oriented mindset, you were likely to make more mistakes since the language and compiler weren’t designed to facilitate that style. 


Enums

Big O notation means the weight is proportional to the speed. 


Electricity Explained
https://m.youtube.com/watch?v=ru032Mfsfig


Great explanation of the "electron sea" model of metals
https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Supplemental_Modules_(Physical_and_Theoretical_Chemistry)/Chemical_Bonding/Fundamentals_of_Chemical_Bonding/Metallic_Bonding

https://chem.libretexts.org/Bookshelves/General_Chemistry/Book%3A_Chem1_(Lower)/09%3A_Chemical_Bonding_and_Molecular_Structure/9.10%3A_Bonding_in_Metals
The high thermal conductivity of metals is attributed to vibrational excitations of the fluid-like electrons; this excitation spreads through the crystal far more rapidly than it does in non-metallic solids which depend on vibrational motions of atoms which are much heavier and possess greater inertia.
With the two exceptions of copper and gold, the closely-spaced levels in the bands allow metals to absorb all wavelengths equally well, so most metals are basically black, but this is ordinarily evident only when the metallic particles are so small that the band structure is not established.

https://en.wikipedia.org/wiki/Period_(periodic_table)
https://en.wikipedia.org/wiki/Electron_configuration
https://en.wikipedia.org/wiki/Electron_shell
Each shell consists of one or more subshells, and each subshell consists of one or more atomic orbitals.
https://en.wikipedia.org/wiki/Atomic_orbital
Atomic orbitals exactly describe the shape of this "atmosphere" only when a single electron is present in an atom. When more electrons are added to a single atom, the additional electrons tend to more evenly fill in a volume of space around the nucleus so that the resulting collection (sometimes termed the atom's "electron cloud"[7]) tends toward a generally spherical zone of probability describing the electron's location, because of the uncertainty principle.
The Bohr model was able to explain the emission and absorption spectra of hydrogen. The energies of electrons in the n = 1, 2, 3, etc. states in the Bohr model match those of current physics. However, this did not explain similarities between different atoms, as expressed by the periodic table, such as the fact that helium (two electrons), neon (10 electrons), and argon (18 electrons) exhibit similar chemical inertness. Modern quantum mechanics explains this in terms of electron shells and subshells which can each hold a number of electrons determined by the Pauli exclusion principle. Thus the n = 1 state can hold one or two electrons, while the n = 2 state can hold up to eight electrons in 2s and 2p subshells. In helium, all n = 1 states are fully occupied; the same is true for n = 1 and n = 2 in neon. In argon, the 3s and 3p subshells are similarly fully occupied by eight electrons; quantum mechanics also allows a 3d subshell but this is at higher energy than the 3s and 3p in argon (contrary to the situation in the hydrogen atom) and remains empty.


Possible start to a script:
The point of a course is not to learn words and their meanings, but to learn concepts. Once you know concepts, you can label them and relabel them without a problem. This is what we do when we learn synonyms. It just becomes a matter of memorizing that new word, not understanding what it means. Once you’ve got a concept in your mind, then it's just a matter of making sure the labels you use for that concept are the same as those used by others. That’s when communication becomes more important than concepts. But communication is not the first goal. Concepts are the first goal. Then communication becomes easy. 

Most courses and books get this wrong. Courses are like books and books only use words and a few sparse images to convey something to you. But words aren’t helpful when you’re trying to learn concepts. Concepts are only available to us in examples. ...
